{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1f8bee8ea2570bec4dab351d9c08c0398d89683"
   },
   "source": [
    "# Introduction\n",
    "* In this kernel, we will learn text mining together step by step. \n",
    "* **Lets start with why we need to learn text mining?**\n",
    "    * Text is everywhere like books, facebook or twitter.\n",
    "    * Text data is growing each passing day. As I read on the internet, amount of text data will be approximately 40 zettabytes(10^21) in 2 years. \n",
    "    * We learn text mining to sentiment analysis, topic modelling, understanding, identfying, finding, classfying and extracting information.\n",
    "<br>Content:\n",
    "1. [Basic Text Mining Methods](#0)\n",
    "     * [len()](#1)\n",
    "     * [split()](#1)\n",
    "     * [istitle()](#2)\n",
    "     * [endswith()](#2)\n",
    "     * [startswith()](#2)\n",
    "     * [set()](#3)\n",
    "     * [isupper()](#4)\n",
    "     * [islower()](#4)\n",
    "     * [isdigit()](#4)\n",
    "     * [strip()](#4)\n",
    "     * [find()](#4)\n",
    "     * [rfind()](#4)\n",
    "     * [replace()](#4)\n",
    "     * [list()](#4)\n",
    "     * [readline()](#5)\n",
    "     * [read()](#5)\n",
    "     * [splitlines()](#5)\n",
    "     * [contains()](#5)\n",
    "     * [Regular Expression Package re](#6)\n",
    "     * [search()](#6)\n",
    "     * [findall()](#7)\n",
    "1. [Natural Language Process](#8)\n",
    "    * [import nltk](#8)\n",
    "    * [FreqDist](#8)\n",
    "    * [Normalization and Stemming words](#9)\n",
    "    * [Lemmatization](#10)\n",
    "    * [Tokenization](#11)\n",
    "1. [Text Classification](#12)    \n",
    "1. [Continue](#13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "230fd1e4857bd46e4e99054cb02bed9a7193ecb8"
   },
   "source": [
    "<a id=\"0\"></a> <br>\n",
    "## Basic Text Mining Methods\n",
    " * Text can be sentences, strings, words, characters and large documents\n",
    " * Now lets create a sentence to understand basics of text mining methods.\n",
    " * Our sentece is \"no woman no cry\" from Bob Marley."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "274b69a23a2d274c7b45212f37fb952633fd0be2"
   },
   "source": [
    "<a id=\"1\"></a> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "4aaf5f3cdfe386125fabfb0abb58bd0b0ff0413e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text:  15\n",
      "Splitted text:  ['No', 'woman', 'no', 'cry']\n"
     ]
    }
   ],
   "source": [
    "# lets create a text\n",
    "text = \"No woman no cry\"\n",
    "\n",
    "# length of text ( includes spaces)\n",
    "print(\"length of text: \",len(text))\n",
    "\n",
    "# split the text\n",
    "splitted_text = text.split() # default split methods splits text according to spaces\n",
    "print(\"Splitted text: \",splitted_text)    # splitted_text is a list that includes words of text sentence\n",
    "# each word is called token in text maning world.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0410821d3cddccc7eb4a6d2e59606130e50f6088"
   },
   "source": [
    "<a id=\"2\"></a> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "97d799c7dacd503ff314ef66861d985b1db194e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words which are more than 3 letter:  ['woman', 'cry']\n",
      "Capitalized words:  ['No']\n",
      "words end with o:  ['No', 'no']\n",
      "words start with w:  ['woman']\n"
     ]
    }
   ],
   "source": [
    "# find specific words with list comprehension method\n",
    "specific_words = [word for word in splitted_text if(len(word)>2)]\n",
    "print(\"Words which are more than 3 letter: \",specific_words)\n",
    "\n",
    "# capitalized words with istitle() method that finds capitalized words\n",
    "capital_words = [ word for word in splitted_text if word.istitle()]\n",
    "print(\"Capitalized words: \",capital_words)\n",
    "\n",
    "# words which end with \"o\": endswith() method finds last letter of word\n",
    "words_end_with_o =  [word for word in splitted_text if word.endswith(\"o\")]\n",
    "print(\"words end with o: \",words_end_with_o) \n",
    "\n",
    "# words which starts with \"w\": startswith() method\n",
    "words_start_with_w = [word for word in splitted_text if word.startswith(\"w\")]\n",
    "print(\"words start with w: \",words_start_with_w) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c7349dcd176633090a431e7a3f760e547f66dc9"
   },
   "source": [
    "<a id=\"3\"></a> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "5de3084612dc757b017147d1e73868f552585451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words:  {'woman', 'No', 'no', 'cry'}\n",
      "unique words:  {'woman', 'cry', 'no'}\n"
     ]
    }
   ],
   "source": [
    "# unique with set() method\n",
    "print(\"unique words: \",set(splitted_text))  # actually the word \"no\" is occured twice bc one word is \"no\" and others \"No\" there is a capital letter at first letter\n",
    "\n",
    "# make all letters lowercase with lower() method\n",
    "lowercase_text = [word.lower() for word in splitted_text]\n",
    "\n",
    "# then find uniques again with set() method\n",
    "print(\"unique words: \",set(lowercase_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "277d130e66c1e0ff842c8bdaa59f9510b8a7b05a"
   },
   "source": [
    "<a id=\"4\"></a> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b78ee7bd5103514ca8714cdb5df92e9960064a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is w letter in woman word: True\n",
      "Is word uppercase: True\n",
      "Is word lowercase: True\n",
      "Is word made of by digits:  True\n",
      "00000000No cry:  No cry\n",
      "Find particular letter from back:  1\n",
      "Find particular letter from back:  8\n",
      "Replace o with 3  N3 cry n3\n",
      "Each letter:  ['N', 'o', ' ', 'c', 'r', 'y']\n"
     ]
    }
   ],
   "source": [
    "# chech words includes or not includes particular substring or letter\n",
    "print(\"Is w letter in woman word:\", \"w\" in \"woman\")\n",
    "\n",
    "# check words are upper case or lower case\n",
    "print(\"Is word uppercase:\", \"WOMAN\".isupper())\n",
    "print(\"Is word lowercase:\", \"cry\".islower())\n",
    "\n",
    "# check words are made of by digits or not\n",
    "print(\"Is word made of by digits: \",\"12345\".isdigit())\n",
    "\n",
    "# get rid of from white space characters like spaces and tabs or from unwanted letters with strip() method\n",
    "print(\"00000000No cry: \",\"00000000No cry\".strip(\"0\"))\n",
    "\n",
    "# find particular letter from front \n",
    "print(\"Find particular letter from back: \",\"No cry no\".find(\"o\"))  # at index 1\n",
    "\n",
    "# find particular letter from back  rfind = reverse find\n",
    "print(\"Find particular letter from back: \",\"No cry no\".rfind(\"o\"))  # at index 8\n",
    "\n",
    "# replace letter with letter\n",
    "print(\"Replace o with 3 \", \"No cry no\".replace(\"o\",\"3\"))\n",
    "\n",
    "# find each letter and store them in list\n",
    "print(\"Each letter: \",list(\"No cry\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "1352213c2e932e92c6e53b37809d925210aecefa",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split text:  ['', '', '', '', 'Be', 'fair', 'and', 'tolerant', '', '', '', '']\n",
      "Cleaned text:  ['Be', 'fair', 'and', 'tolerant']\n"
     ]
    }
   ],
   "source": [
    "# Cleaning text\n",
    "text1 = \"    Be fair and tolerant    \"\n",
    "print(\"Split text: \",text1.split(\" \"))   # as you can see there are unnecessary white space in list\n",
    "\n",
    "# get rid of from these unnecassary white spaces with strip() method then split\n",
    "print(\"Cleaned text: \",text1.strip().split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "373c53e00457b0929e12d550ef1f680b62c4d2ff"
   },
   "source": [
    "<a id=\"5\"></a> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "3d857c1704e668528b980ef65b70d532f958ed0c"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/religious-and-philosophical-texts/35895-0.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10566/872101164.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reading files line by line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/religious-and-philosophical-texts/35895-0.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# read first line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/religious-and-philosophical-texts/35895-0.txt'"
     ]
    }
   ],
   "source": [
    "# reading files line by line\n",
    "f = open(\"../input/religious-and-philosophical-texts/35895-0.txt\",\"r\")\n",
    "\n",
    "# read first line\n",
    "print(f.readline())\n",
    "\n",
    "# length of text\n",
    "text3=f.read()\n",
    "print(\"Length of text: \",len(text3))\n",
    "\n",
    "# Number of lines with splitlines() method\n",
    "lines = text3.splitlines()\n",
    "print(\"Number of lines: \",len(lines))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "9120ded28f063e7d164909a8a8c8af4802c8353d"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/ben-hamners-tweets/benhamner.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10566/2427557125.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"../input/ben-hamners-tweets/benhamner.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/ben-hamners-tweets/benhamner.csv'"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "data = pd.read_csv(r\"../input/ben-hamners-tweets/benhamner.csv\", encoding='latin-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "59c2c50ae9bc0e78831a69efcd4be876109fbe45"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10566/524592417.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# find which entries contain the word 'appointment'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In his tweets, the rate of occuring kaggle word is: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kaggle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# find which entries contain the word 'appointment'\n",
    "print(\"In his tweets, the rate of occuring kaggle word is: \",sum(data.text.str.contains('kaggle'))/len(data))\n",
    "# text\n",
    "text = data.text[1]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa0e587f8148155c401577e804c3a85f76f5e18c"
   },
   "source": [
    "<a id=\"6\"></a> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e7ca63da940c23e9d4098f4d8a9a489c77e0651"
   },
   "outputs": [],
   "source": [
    "# find regular expression on text\n",
    "# import regular expression package\n",
    "import re\n",
    "# find callouts that starts with @\n",
    "callouts = [word for word in text.split(\" \") if re.search(\"@[A-Za-z0-9_]+\",word)]\n",
    "print(\"callouts: \",callouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b76990b47417e0b8ed2687849763518d54131855"
   },
   "source": [
    "* Lets look at this @[A-Za-z0-9_]+ expression more detailed\n",
    "    * @: we say that our searched word start with @\n",
    "    * [A-Za-z0-9_]: @ is followed by upper or lower case letters, digits or underscore\n",
    "    * +: there can be more than one @. So with \"+\" sign we say that the words which start with @ can be occured more than one times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a9bed92566f8f89a528d6dae7fa868ce96029e4"
   },
   "outputs": [],
   "source": [
    "# continue finding regular expressions\n",
    "# [A-Za-z0-9_] =\\w\n",
    "# We will use \"\\w\" to find callouts and our result will be same because \\w matches with [A-Za-z0-9_] \n",
    "callouts1 = [word for word in text.split(\" \") if re.search(\"@\\w+\",word)]\n",
    "print(\"callouts: \",callouts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "264778d4ff52535ef72c9e15e93c3810fb7d387a"
   },
   "source": [
    "<a id=\"7\"></a> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76085e59be9195f2b74b5969ee38719bee33859e"
   },
   "outputs": [],
   "source": [
    "# find specific characters like \"w\"\n",
    "print(re.findall(r\"[w]\",text))\n",
    "# \"w\"ith, \"w\"indo\"w\", sho\"w\"ing, s\"w\"itches \n",
    "\n",
    "# do not find specific character like \"w\". We will use \"^\" symbol\n",
    "print(re.findall(r\"[^w]\",text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1b4d0b3090c0c508fd9896f1b7040c135236a204"
   },
   "source": [
    "* Lets look at regular expressions for date\n",
    "* We will use \"\\d{1,2}[/-]\\d{1,2}[/-]\\d{1,4}\" expression to find dates\n",
    "    * d{1,2}: first number can be 1 or 2 digit\n",
    "    * [/-]: between digits there can be \"/\" or \"-\" symbols\n",
    "    * d{1,4}: last number can be between 1 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53dc78ef29b7f087ed73fa18150a568e1e678860"
   },
   "outputs": [],
   "source": [
    "# Regular expressions for Dates\n",
    "date = \"15-10-2000\\n09/10/2005\\n15-05-1999\\n05/05/99\\n\\n05/05/199\\n\\n05/05/9\"\n",
    "re.findall(r\"\\d{1,2}[/-]\\d{1,2}[/-]\\d{1,4}\",date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e75a144ddd3ec6596c2f52a89bf7702df302feb"
   },
   "source": [
    "<a id=\"8\"></a> <br>\n",
    "## Natural Language Process (NLP)\n",
    "* Natural language is any language that is used by people in everyday like English or Spanish\n",
    "* Natural language processing is that any computation and manipulation of natural language to get inside about how words mean and how sentences are contructed is natural language processing.\n",
    "* Natural languages are in change like new words tweets\n",
    "* Natural language process tasks are counting words, finding unique words and sentence boundaries, identify semantic rules and entities in a sentence\n",
    "* We will use natural language tool kit that is open source library in python.\n",
    "    * It supports most of the NLP tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de1458d10de38b7af7ded327d61be9225ddd56d3"
   },
   "outputs": [],
   "source": [
    "# import natural language tool kit\n",
    "import nltk as nlp\n",
    "\n",
    "# counting vocabulary of words\n",
    "text = data.text[1]\n",
    "splitted = text.split(\" \")\n",
    "print(\"number of words: \",len(splitted))\n",
    "\n",
    "# counting unique vocabulary of words\n",
    "text = data.text[1]\n",
    "print(\"number of unique words: \",len(set(splitted)))\n",
    "\n",
    "# print first five unique words\n",
    "print(\"first 5 unique words: \",list(set(splitted))[:5])\n",
    "\n",
    "# frequency of words \n",
    "dist = nlp.FreqDist(splitted)\n",
    "print(\"frequency of words: \",dist)\n",
    "\n",
    "# look at keys in dist\n",
    "print(\"words in text: \",dist.keys())\n",
    "\n",
    "# count how many time a particalar value occurs. Lets look at \"box\"\n",
    "print(\"the word box is occured how many times:\",dist[\"box\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d033fe87cec8adb5d7b77b3c438000aae72163d1"
   },
   "source": [
    "<a id=\"9\"></a> <br>\n",
    "## Normalization and Stemming words\n",
    "* Normalization is different forms of the same word like have and having\n",
    "* Stemming is finding a root of the words like having => have\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fc58b5041d135fafc5a2bff10e3cd72e4d84c895"
   },
   "outputs": [],
   "source": [
    "# normalization\n",
    "words = \"task Tasked tasks tasking\"\n",
    "words_list = words.lower().split(\" \")\n",
    "print(\"normalized words: \",words_list)\n",
    "\n",
    "# stemming\n",
    "porter_stemmer = nlp.PorterStemmer()\n",
    "roots = [porter_stemmer.stem(each) for each in words_list]\n",
    "print(\"roots of task Tasked tasks tasking: \",roots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff8bbe08d0b42ca72cb1e47e626527bad07d7d7a",
    "collapsed": true
   },
   "source": [
    "<a id=\"10\"></a> <br>\n",
    "## Lemmatization\n",
    "* It is also stemming but resulting stems are all valid words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "902265ac9ba0a6d1ef27dfed933eb46ca8f5434c"
   },
   "outputs": [],
   "source": [
    "# stemming\n",
    "stemming_word_list = [\"Universal\",\"recognition\",\"Become\",\"being\",\"happened\"]\n",
    "porter_stemmer = nlp.PorterStemmer()\n",
    "roots = [porter_stemmer.stem(each) for each in stemming_word_list]\n",
    "print(\"result of stemming: \",roots)\n",
    "\n",
    "# lemmatization\n",
    "lemma = nlp.WordNetLemmatizer()\n",
    "lemma_roots = [lemma.lemmatize(each) for each in stemming_word_list]\n",
    "print(\"result of lemmatization: \",lemma_roots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac7a394d97e0fa295e052733fd77608db20d3d41"
   },
   "source": [
    "<a id=\"11\"></a> <br>\n",
    "## Tokenization\n",
    "* Splitting a sentece into words(tokes)\n",
    "* Learn tokenize with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29582aada7b17bba4f23ab76641b7252e8b95368"
   },
   "outputs": [],
   "source": [
    "text_t = \"Youâ€™re in the right place!\"\n",
    "print(\"split the sentece: \", text_t.split(\" \"))  # 5 words\n",
    "\n",
    "# tokenization with nltk\n",
    "print(\"tokenize with nltk: \",nlp.word_tokenize(text_t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35df7948a79bf3b774e2f8dcd61b209a38540bfe"
   },
   "source": [
    "<a id=\"12\"></a> <br>\n",
    "# Text Classification\n",
    "*  Classify male and femala according to their tweets(description)\n",
    "* import twitter data set from \"Twitter User Gender Classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33ffff53a5a166b92fc43f8252aaee1bd9074ce4"
   },
   "outputs": [],
   "source": [
    "# %% import data\n",
    "data = pd.read_csv(r\"../input/twitter-user-gender-classification/gender-classifier-DFE-791531.csv\",encoding='latin1')\n",
    "\n",
    "# concat gender and description\n",
    "data = pd.concat([data.gender,data.description],axis=1)\n",
    "\n",
    "# drop nan values\n",
    "data.dropna(inplace=True,axis=0)\n",
    "\n",
    "# convert genders from female and male to 1 and 0 respectively\n",
    "data.gender = [1 if each == \"female\" else 0 for each in data.gender] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47eb761ea96d61cf38372073cbf57cc38bb839c5"
   },
   "outputs": [],
   "source": [
    "# import re # regular expression library\n",
    "# # %% remove non important word a, the, that, and, in \n",
    "# import nltk as nlp\n",
    "# import nltk\n",
    "# nltk.download(\"stopwords\")  # stopwords = (irrelavent words)\n",
    "# from nltk.corpus import stopwords \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "144258d2fc56c53d68a76b10042572063c82b521"
   },
   "outputs": [],
   "source": [
    "# %% creating bag of words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # for bag of words \n",
    "max_features = 150 # max_features dimension reduction \n",
    "count_vectorizer = CountVectorizer(stop_words = \"english\",max_features = max_features)  \n",
    "# stop_words parameter = automatically remove all stopwords \n",
    "# lowercase parameter \n",
    "# token_pattern removing other karakters like .. !\n",
    "\n",
    "sparce_matrix = count_vectorizer.fit_transform(review_list).toarray() # sparce matrix yaratir bag of words model = sparce matrix\n",
    "\n",
    "print(\"Most used {} words: {}\".format(max_features,count_vectorizer.get_feature_names()))\n",
    "\n",
    "y = data.iloc[:,0].values  # positive or negative comment\n",
    "\n",
    "#sparce matrix includes independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "627fd0ca8b9c9ebf9966d1bebf307243ca3b92e1"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(sparce_matrix,y,test_size = 0.1,random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d30dc0c6970ff75b329ad12ca6cd117a513e9e1a"
   },
   "source": [
    "* As a classifier we use naive bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "636af2788fbbf7323fa5311607fcb5fcd5c6a9d3"
   },
   "outputs": [],
   "source": [
    "# %% naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(sparce_matrix,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "610196f8904d8e86540388836a38203d68fe7cf7"
   },
   "outputs": [],
   "source": [
    "# %% predict\n",
    "y_pred = nb.predict(sparce_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a19eecab94635b192fece6c996d101b1906ccde5"
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4291363543ad57bab64708e1fe7460fd00f34dd9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "f,ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(cm, annot=True, linewidths=0.5,linecolor=\"red\", fmt= '.1f',ax=ax)\n",
    "plt.show()\n",
    "plt.savefig('graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "475fde7c772a3ba4466de2767340b1da9488e04c"
   },
   "source": [
    "* The result is not good but you got the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "<a id=\"13\"></a> <br>\n",
    "# Conclusion\n",
    "* If you have any question you can free to ask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
